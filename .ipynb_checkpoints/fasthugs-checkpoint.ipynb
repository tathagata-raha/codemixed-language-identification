{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exposed-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "separated-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastai.basics import *\n",
    "from fastai.text.all import *\n",
    "from fastai.callback.all import *\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "from splitters import *\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lightweight-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastHugsTokenizer():\n",
    "    \"\"\" \n",
    "        transformer_tokenizer : takes the tokenizer that has been loaded from the tokenizer class\n",
    "        model_name : model type set by the user\n",
    "        max_seq_len : override default sequence length, typically 512 for bert-like models.\n",
    "                           `transformer_tokenizer.max_len_single_sentence` and `transformer_tokenizer.max_len_sentences_pair` \n",
    "                           both account for the need to add additional special tokens, i.e. for RoBERTa-base \n",
    "                           max_len_single_sentence==510, leaving space for the 2 additional special tokens \n",
    "                           to be added for the model's default 512 positional embeddings\n",
    "        pair : whether a single sentence (sequence) or pair of sentences are used\n",
    "        \n",
    "        NOTES:\n",
    "            - `init` will have to be modified to enable sequence lengths larger than the tokenizer default\n",
    "            - need to add case when pretrained==False\n",
    "            - Pretrained==True will cut the sequence at the max length\n",
    "            - Good functions in `tokenization_utils.py`\n",
    "            - tokenizer.encode_plus or tokenizer.batch_encode_plus are great, but don't play nice with fastai multiprocessiing\n",
    "            - https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus\n",
    "            - encoded_dict=tokenizer.encode_plus(text=o, return_tensors=\"pt\", max_length=tokenizer.max_len, pad_to_max_length=True)\n",
    "        Returns:\n",
    "            - Tokenized text, up to the max sequence length set by the user or the tokenzier default\n",
    "    \"\"\"\n",
    "    def __init__(self, transformer_tokenizer=None, model_name='xlm-roberta-base', max_seq_len=None, \n",
    "                 pretrained=True, pair=False, **kwargs): \n",
    "        self.model_name, self.tok, self.max_seq_len=model_name, transformer_tokenizer, max_seq_len\n",
    "        if pretrained:\n",
    "            if self.max_seq_len:\n",
    "                if pair: assert self.max_seq_len<=self.tok.max_len_sentences_pair, 'WARNING: max_seq_len needs to be less than or equal to transformer_tokenizer.max_len_sentences_pair'\n",
    "                else: assert self.max_seq_len<=self.tok.max_len_single_sentence, 'WARNING: max_seq_len needs to be less than or equal to transformer_tokenizer.max_len_single_sentence'\n",
    "            else:\n",
    "                if pair: self.max_seq_len=ifnone(max_seq_len, self.tok.max_len_sentences_pair) \n",
    "                else: self.max_seq_len=ifnone(max_seq_len, self.tok.max_len_single_sentence)\n",
    "\n",
    "    def do_tokenize(self, o:str):\n",
    "        \"\"\"Returns tokenized text, adds prefix space if needed, limits the maximum sequence length\"\"\"\n",
    "        if 'roberta' in model_name: tokens=self.tok.tokenize(o, add_prefix_space=True)[:self.max_seq_len-2]\n",
    "        else: tokens = self.tok.tokenize(o)[:self.max_seq_len-2]\n",
    "        return tokens\n",
    "    def __call__(self, items): \n",
    "        for o in items: yield self.do_tokenize(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unlikely-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastHugsModel(nn.Module):\n",
    "    'Inspired by https://www.kaggle.com/melissarajaram/roberta-fastai-huggingface-transformers/data'\n",
    "    def __init__(self, transformer_cls, tokenizer, config_dict, n_class, pretrained=True):\n",
    "        super(FastHugsModel, self).__init__()\n",
    "        self.tok, self.config, self.config._num_labels = tokenizer, config_dict, n_class\n",
    "        # load model\n",
    "        if pretrained: self.transformer = transformer_cls.from_pretrained(model_name, config=self.config)\n",
    "        else: self.transformer = transformer_cls.from_config(config=self.config)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        attention_mask = (input_ids!=self.tok.pad_token_id).type(input_ids.type())\n",
    "        logits = self.transformer(input_ids, attention_mask = attention_mask)[0] \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "responsible-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xlm-roberta-base' \n",
    "model_class = AutoModelForSequenceClassification\n",
    "config_dict = AutoConfig.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incomplete-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "forced-norman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "early-memorial",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'splitters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-899868e4d56b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msplitter_nm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'roberta_cls_splitter'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_splitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplitter_nm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'splitters' is not defined"
     ]
    }
   ],
   "source": [
    "splitter_nm = 'roberta_cls_splitter'\n",
    "model_splitter = splittersx[splitter_nm]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-graphics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-freeware",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
