{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "coral-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.datasets import *\n",
    "from fastai.text.data import *\n",
    "from fastai.text.learner import *\n",
    "from fastai.text.transform import *\n",
    "from fastai import *\n",
    "from fastai.text.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "latter-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "terminal-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.pkl', 'rb') as f:\n",
    "    tokens = pickle.load(f)\n",
    "train = pd.read_csv('train.csv')\n",
    "valid = pd.read_csv('valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "racial-withdrawal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['holy',\n",
       " 'league',\n",
       " 'battle',\n",
       " 'of',\n",
       " 'vienna',\n",
       " 'cardinal',\n",
       " 'richelieu',\n",
       " 'louis',\n",
       " 'xiv',\n",
       " 'of',\n",
       " 'france']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "false-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab.create(tokens, max_vocab = 45000, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aggregate-costa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like\n"
     ]
    }
   ],
   "source": [
    "print(vocab.textify(nums=[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "linear-metallic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = TextClasDataBunch.from_df(path = '.', train_df = train, valid_df =valid, text_cols='text', label_cols='lang', vocab=vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "blank-citizenship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (18211 items)\n",
       "x: TextList\n",
       "xxbos ittymani fansin pokunnavar ivde like chey,xxbos neraya neraya neraya intha padaththula xxunk ... thala ...,xxbos awards for janine turner from the internet movie database,xxbos enthaa trailer pathaa yaaruku kgf flash avudhu,xxbos do nt worry mil jaega\n",
       "y: CategoryList\n",
       "4,3,1,3,2\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (4553 items)\n",
       "x: TextList\n",
       "xxbos german and sioux indian ancestry,xxbos wang mang still refused to let her visit the capital,xxbos yaarlam ajith sir a thavara vera yaaralayum indha role la panna mudiyadhu nu ninaikkireenga,xxbos unfortunately hot springs can create ideal conditions to spread infections,xxbos flop kaluda rajakumaran ee padam hit agum pisharadikku\n",
       "y: CategoryList\n",
       "1,1,3,1,4\n",
       "Path: .;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "spoken-sending",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.60'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fastai\n",
    "fastai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "reflected-aviation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (18211 items)\n",
       "x: TextList\n",
       "xxbos ittymani fansin pokunnavar ivde like chey,xxbos neraya neraya neraya intha padaththula xxunk ... thala ...,xxbos awards for janine turner from the internet movie database,xxbos enthaa trailer pathaa yaaruku kgf flash avudhu,xxbos do nt worry mil jaega\n",
       "y: CategoryList\n",
       "4,3,1,3,2\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (4553 items)\n",
       "x: TextList\n",
       "xxbos german and sioux indian ancestry,xxbos wang mang still refused to let her visit the capital,xxbos yaarlam ajith sir a thavara vera yaaralayum indha role la panna mudiyadhu nu ninaikkireenga,xxbos unfortunately hot springs can create ideal conditions to spread infections,xxbos flop kaluda rajakumaran ee padam hit agum pisharadikku\n",
       "y: CategoryList\n",
       "1,1,3,1,4\n",
       "Path: .;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "loose-secondary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method fit_one_cycle of RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (18211 items)\n",
       "x: TextList\n",
       "xxbos ittymani fansin pokunnavar ivde like chey,xxbos neraya neraya neraya intha padaththula xxunk ... thala ...,xxbos awards for janine turner from the internet movie database,xxbos enthaa trailer pathaa yaaruku kgf flash avudhu,xxbos do nt worry mil jaega\n",
       "y: CategoryList\n",
       "4,3,1,3,2\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (4553 items)\n",
       "x: TextList\n",
       "xxbos german and sioux indian ancestry,xxbos wang mang still refused to let her visit the capital,xxbos yaarlam ajith sir a thavara vera yaaralayum indha role la panna mudiyadhu nu ninaikkireenga,xxbos unfortunately hot springs can create ideal conditions to spread infections,xxbos flop kaluda rajakumaran ee padam hit agum pisharadikku\n",
       "y: CategoryList\n",
       "1,1,3,1,4\n",
       "Path: .;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): TransformerXL(\n",
       "      (encoder): Embedding(43704, 410)\n",
       "      (pos_enc): PositionalEncoding()\n",
       "      (drop_emb): Dropout(p=0.05, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.05, inplace=False)\n",
       "      (2): Linear(in_features=1230, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x1507e0451820>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(43704, 410)\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): ParameterModule()\n",
       "  (5): ParameterModule()\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.05, inplace=False)\n",
       "      (2): Linear(in_features=1230, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = text_classifier_learner(data, TransformerXL, drop_mult=0.5)\n",
    "learn.fit_one_cycle\n",
    "learn.fit_one_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "technological-fleet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.1'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fastprogress\n",
    "fastprogress.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "vietnamese-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "broad-alliance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Category 1, tensor(0), tensor([0.2651, 0.2526, 0.2422, 0.2400]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict('zxzczx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-relationship",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
