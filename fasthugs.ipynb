{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "answering-buyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "incident-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastai.basics import *\n",
    "from fastai.text.all import *\n",
    "from fastai.callback.all import *\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "# import splitters\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "detected-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastHugsTokenizer():\n",
    "    \"\"\" \n",
    "        transformer_tokenizer : takes the tokenizer that has been loaded from the tokenizer class\n",
    "        model_name : model type set by the user\n",
    "        max_seq_len : override default sequence length, typically 512 for bert-like models.\n",
    "                           `transformer_tokenizer.max_len_single_sentence` and `transformer_tokenizer.max_len_sentences_pair` \n",
    "                           both account for the need to add additional special tokens, i.e. for RoBERTa-base \n",
    "                           max_len_single_sentence==510, leaving space for the 2 additional special tokens \n",
    "                           to be added for the model's default 512 positional embeddings\n",
    "        pair : whether a single sentence (sequence) or pair of sentences are used\n",
    "        \n",
    "        NOTES:\n",
    "            - `init` will have to be modified to enable sequence lengths larger than the tokenizer default\n",
    "            - need to add case when pretrained==False\n",
    "            - Pretrained==True will cut the sequence at the max length\n",
    "            - Good functions in `tokenization_utils.py`\n",
    "            - tokenizer.encode_plus or tokenizer.batch_encode_plus are great, but don't play nice with fastai multiprocessiing\n",
    "            - https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus\n",
    "            - encoded_dict=tokenizer.encode_plus(text=o, return_tensors=\"pt\", max_length=tokenizer.max_len, pad_to_max_length=True)\n",
    "        Returns:\n",
    "            - Tokenized text, up to the max sequence length set by the user or the tokenzier default\n",
    "    \"\"\"\n",
    "    def __init__(self, transformer_tokenizer=None, model_name='xlm-roberta-base', max_seq_len=None, \n",
    "                 pretrained=True, pair=False, **kwargs): \n",
    "        self.model_name, self.tok, self.max_seq_len=model_name, transformer_tokenizer, max_seq_len\n",
    "        if pretrained:\n",
    "            if self.max_seq_len:\n",
    "                if pair: assert self.max_seq_len<=self.tok.max_len_sentences_pair, 'WARNING: max_seq_len needs to be less than or equal to transformer_tokenizer.max_len_sentences_pair'\n",
    "                else: assert self.max_seq_len<=self.tok.max_len_single_sentence, 'WARNING: max_seq_len needs to be less than or equal to transformer_tokenizer.max_len_single_sentence'\n",
    "            else:\n",
    "                if pair: self.max_seq_len=ifnone(max_seq_len, self.tok.max_len_sentences_pair) \n",
    "                else: self.max_seq_len=ifnone(max_seq_len, self.tok.max_len_single_sentence)\n",
    "\n",
    "    def do_tokenize(self, o:str):\n",
    "        \"\"\"Returns tokenized text, adds prefix space if needed, limits the maximum sequence length\"\"\"\n",
    "        if 'roberta' in model_name: tokens=self.tok.tokenize(o)[:self.max_seq_len-2]\n",
    "        else: tokens = self.tok.tokenize(o)[:self.max_seq_len-2]\n",
    "        return tokens\n",
    "    def __call__(self, items): \n",
    "        for o in items: yield self.do_tokenize(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "north-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastHugsModel(nn.Module):\n",
    "    'Inspired by https://www.kaggle.com/melissarajaram/roberta-fastai-huggingface-transformers/data'\n",
    "    def __init__(self, transformer_cls, tokenizer, config_dict, n_class, pretrained=True):\n",
    "        super(FastHugsModel, self).__init__()\n",
    "        self.tok, self.config, self.config._num_labels = tokenizer, config_dict, n_class\n",
    "        # load model\n",
    "        if pretrained: self.transformer = transformer_cls.from_pretrained(model_name, config=self.config)\n",
    "        else: self.transformer = transformer_cls.from_config(config=self.config)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        attention_mask = (input_ids!=self.tok.pad_token_id).type(input_ids.type())\n",
    "        logits = self.transformer(input_ids, attention_mask = attention_mask)[0] \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "decent-estimate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e337b3c147e949dd99dc4aa74fad31c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/512 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'xlm-roberta-base' \n",
    "model_class = AutoModelForSequenceClassification\n",
    "config_dict = AutoConfig.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "crazy-auditor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73296aa91af1411798abb727e9a06b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed4842d18704a5b9ad47887f29a7e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "250002"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_vocab=tokenizer.get_vocab() \n",
    "tokenizer_vocab_ls = [k for k, v in sorted(tokenizer_vocab.items(), key=lambda item: item[1])]\n",
    "len(tokenizer_vocab_ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "electrical-wealth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "surface-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_cls_splitter(m):\n",
    "    \"Split the classifier head from the backbone\"\n",
    "    groups = [nn.Sequential(m.transformer.roberta.embeddings,\n",
    "                  m.transformer.roberta.encoder.layer[0],\n",
    "                  m.transformer.roberta.encoder.layer[1],\n",
    "                  m.transformer.roberta.encoder.layer[2],\n",
    "                  m.transformer.roberta.encoder.layer[3],\n",
    "                  m.transformer.roberta.encoder.layer[4],\n",
    "                  m.transformer.roberta.encoder.layer[5],\n",
    "                  m.transformer.roberta.encoder.layer[6],\n",
    "                  m.transformer.roberta.encoder.layer[7],\n",
    "                  m.transformer.roberta.encoder.layer[8],\n",
    "                  m.transformer.roberta.encoder.layer[9],\n",
    "                  m.transformer.roberta.encoder.layer[10],\n",
    "                  m.transformer.roberta.encoder.layer[11],\n",
    "                  m.transformer.roberta.pooler)]\n",
    "    groups = L(groups + [m.transformer.classifier])\n",
    "    return groups.map(params)\n",
    "model_splitter = roberta_cls_splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "voluntary-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 512  \n",
    "sentence_pair=False\n",
    "\n",
    "fasthugstok = partial(FastHugsTokenizer, transformer_tokenizer=tokenizer, model_name=model_name, \n",
    "                      max_seq_len=max_seq_len, sentence_pair=sentence_pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bibliographic-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai_tokenizer = Tokenizer.from_df(text_cols='text', res_col_name='text', tok_func=fasthugstok, \n",
    "                                     rules=[], post_rules=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "headed-superior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastai_tokenizer.rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "backed-fundamentals",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialClsTokens(Transform):\n",
    "    \"Add special token_ids to the numericalized tokens for Sequence Classification\"\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tok=tokenizer\n",
    "    def encodes(self, o):\n",
    "        return(TensorText(self.tok.build_inputs_with_special_tokens(list(o))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "stupid-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=[\"If you want the real version of this over blown American clown act, watch William Wylers' 1944 version - the true story of the 'Memphis Belle'. It's amazing what Hollywood will do to distort history and mock its' veterans, all for a buck. Well it must be the American way! Younger viewers will be beguiled by the nonsense, however older viewers with some sense of history will recognize this movie for what it is worth. Don't waste your time! However, if you don't want the truth, then put your mind in neutral and watch this movie.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "hindu-fault",
   "metadata": {},
   "outputs": [],
   "source": [
    "fht=FastHugsTokenizer(transformer_tokenizer=tokenizer, model_name='roberta', max_seq_len=256, \n",
    "                 pretrained=True, pair=False)\n",
    "tokenized_text = next(fht(txt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "split-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(Numericalize(vocab=tokenizer_vocab_ls)(tokenized_text),\n",
    "        TensorText(tokenizer.convert_tokens_to_ids(tokenized_text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "tired-mentor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_special length: 135, with_special length: 137\n"
     ]
    }
   ],
   "source": [
    "pre_special=Numericalize(vocab=tokenizer_vocab_ls)(tokenized_text)\n",
    "with_special=SpecialClsTokens(tokenizer)(pre_special)\n",
    "print(f'pre_special length: {len(pre_special)}, with_special length: {len(with_special)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "enhanced-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "vulnerable-forth",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'is_valid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/envs/p3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'is_valid'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-848ec3ddccde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mColSplitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_tfms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mattrgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastai_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer_vocab_ls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpecialClsTokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#x_tfms = [attrgetter(\"text\"), fastai_tokenizer, DumTfm(tokenizer)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_tfms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mattrgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSortedDL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/p3/lib/python3.9/site-packages/fastai/data/transforms.py\u001b[0m in \u001b[0;36m_inner\u001b[0;34m(o)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ColSplitter only works when your items are a pandas DataFrame\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mvalid_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bool'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mIndexSplitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask2idxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/p3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/p3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'is_valid'"
     ]
    }
   ],
   "source": [
    "splits = ColSplitter()(df)\n",
    "x_tfms = [attrgetter(\"text\"), fastai_tokenizer, Numericalize(vocab=tokenizer_vocab_ls), SpecialClsTokens(tokenizer)]\n",
    "\n",
    "#x_tfms = [attrgetter(\"text\"), fastai_tokenizer, DumTfm(tokenizer)]\n",
    "dsets = Datasets(df, splits=splits, tfms=[x_tfms, [attrgetter(\"label\"), Categorize()]], dl_type=SortedDL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-wound",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
